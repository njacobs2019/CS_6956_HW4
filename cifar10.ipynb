{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from diffusers import UNet2DModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    image_size: int = 32\n",
    "    train_batch_size: int = 256\n",
    "    eval_batch_size: int = 16  # how many images to sample during evaluation\n",
    "    num_epochs: int = 20  # 50\n",
    "    gradient_accumulation_steps: int = 1\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_warmup_steps: int = 500\n",
    "    save_image_epochs: int = 10\n",
    "    save_model_epochs: int = 30\n",
    "    mixed_precision: str = (\n",
    "        \"fp16\"  # `no` for float32, `fp16` for automatic mixed precision\n",
    "    )\n",
    "    device: torch.device = torch.device(\"cuda:1\")\n",
    "    output_dir: str = (\n",
    "        \"cifar10-unconditional\"  # the model name locally and on the HF Hub\n",
    "    )\n",
    "\n",
    "    push_to_hub: bool = False  # whether to upload the saved model to the HF Hub\n",
    "    # hub_model_id = \"<your-username>/<my-awesome-model>\"  # the name of the repository to create on the HF Hub\n",
    "    # hub_private_repo = None\n",
    "    overwrite_output_dir: bool = (\n",
    "        True  # overwrite the old model when re-running the notebook\n",
    "    )\n",
    "    seed: int = 0\n",
    "\n",
    "\n",
    "config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"./datasets\")\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=, std=), # ???????\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=root, train=True, transform=transform, download=True)\n",
    "test_ds = datasets.CIFAR10(root=root, train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=config.train_batch_size, shuffle=True, num_workers=16\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=config.eval_batch_size, shuffle=False, num_workers=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet2DModel(\n",
    "    sample_size=config.image_size,  # the target image resolution\n",
    "    in_channels=1,  # the number of input channels, 3 for RGB images\n",
    "    out_channels=1,  # the number of output channels\n",
    "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "    block_out_channels=(\n",
    "        64,\n",
    "        64,\n",
    "        128,\n",
    "        128,\n",
    "        256,\n",
    "        # 128,\n",
    "        # 128,\n",
    "        # 256,\n",
    "        # 256,\n",
    "        # 512,\n",
    "        # 512,\n",
    "    ),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "        # \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"DownBlock2D\",\n",
    "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpBlock2D\",\n",
    "        # \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "        \"UpBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self, my_model):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.model = my_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, timestep=torch.tensor([1.0]).to(config.device))\n",
    "\n",
    "\n",
    "sm = SimpleModel(model)\n",
    "\n",
    "for batch in train_loader:\n",
    "    img1, _ = batch\n",
    "    break\n",
    "\n",
    "model = model.to(config.device)\n",
    "img1 = img1.to(config.device)\n",
    "ts = torch.tensor([1.0]).to(config.device)\n",
    "\n",
    "model(img1, timestep=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = sm.to(config.device)\n",
    "sm(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(sm, (1, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from PIL import Image\n",
    "from diffusers import DDPMScheduler\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "# noise = torch.randn(sample_image.shape)\n",
    "timesteps = torch.LongTensor([50])\n",
    "# noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)\n",
    "\n",
    "# Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=config.learning_rate, foreach=False\n",
    ")\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=config.lr_warmup_steps,\n",
    "    num_training_steps=(len(train_loader) * config.num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from diffusers import DDPMPipeline\n",
    "from diffusers.utils import make_image_grid\n",
    "\n",
    "\n",
    "def evaluate(config, epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size=config.eval_batch_size,\n",
    "        generator=torch.Generator(device=\"cpu\").manual_seed(\n",
    "            config.seed\n",
    "        ),  # Use a separate torch generator to avoid rewinding the random state of the main training loop\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_image_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(config.output_dir, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# from huggingface_hub import create_repo, upload_folder\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler\n",
    "):\n",
    "    global_step = 0\n",
    "\n",
    "    model.eval()\n",
    "    model.to(config.device)\n",
    "\n",
    "    pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "\n",
    "    # Now you train the model\n",
    "    for epoch in range(config.num_epochs):\n",
    "        # progress_bar = tqdm(\n",
    "        #     total=len(train_dataloader), # disable=not accelerator.is_local_main_process\n",
    "        # )\n",
    "        # progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        for step, batch in tqdm(\n",
    "            enumerate(train_dataloader), leave=False, total=len(train_dataloader)\n",
    "        ):\n",
    "            with torch.no_grad():\n",
    "                clean_images, _labels = batch\n",
    "                clean_images = clean_images.to(config.device)\n",
    "\n",
    "                # Sample noise to add to the images\n",
    "                noise = torch.randn(clean_images.shape, device=clean_images.device)\n",
    "                batch_size = clean_images.shape[0]\n",
    "\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (batch_size,),\n",
    "                    # device=clean_images.device,\n",
    "                    dtype=torch.int64,\n",
    "                ).to(config.device)\n",
    "\n",
    "                # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_images = noise_scheduler.add_noise(\n",
    "                    clean_images, noise, timesteps\n",
    "                ).to(config.device)\n",
    "\n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            # print(f\"step: {step}\")\n",
    "\n",
    "            # progress_bar.update(1)\n",
    "            # logs = {\n",
    "            #     \"loss\": loss.detach().item(),\n",
    "            #     \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            #     \"step\": global_step,\n",
    "            # }\n",
    "            # progress_bar.set_postfix(**logs)\n",
    "            global_step += 1\n",
    "\n",
    "        if epoch % 3 == 0:\n",
    "            evaluate(config, epoch, pipeline)\n",
    "        print(f\"Finished epoch {epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(config, model, noise_scheduler, optimizer, train_loader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying inference\n",
    "\n",
    "noise_scheduler2 = DDPMScheduler(num_train_timesteps=1000)\n",
    "\n",
    "pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler2)\n",
    "\n",
    "\n",
    "evaluate(config, 3, pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PDL_HW4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
